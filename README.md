**GPT-2 Paper Replication**
======================

This repository contains the code and resources used to replicate the [1] "Language Models are Few-Shot Learners" paper, specifically the GPT-2 (Generative Pre-training of Transformers) model.

**Features:**

* PyTorch implementation of the GPT-2 architecture
* Replication of most of the original paper's hyperparameters and training procedure
* Training data: FineWeb (https://huggingface.co/datasets/HuggingFaceFW/fineweb)
* Evaluation metrics: HellaSwag (https://rowanzellers.com/hellaswag/)

**Note:** This project is intended for educational and research purposes only. The original GPT-2 paper and its associated intellectual property rights are owned by their respective authors.

**References:**

[1] "Language Models are Few-Shot Learners" (2019), (https://arxiv.org/abs/2005.14165)
